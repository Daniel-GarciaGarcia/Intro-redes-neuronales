{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial NN-CNN\n",
    "\n",
    "![](neural.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- ¿Qué son las redes neuronales?\n",
    "\n",
    "Las redes neuronales son un tipo de modelo de machine learning que están diseñadas para operar de manera similar a las neuronas biológicas y el sistema nervioso humano. Estos modelos se usan para reconocer patrones complejos y relaciones que existen en un dataset con etiquetas. Tienen las siguientes propiedades:\n",
    "\n",
    "    La arquitectura de un modelo de red neuronal está compuesta de un gran numero de nodos simples de procesamiento, llamados neuronas,  las cuales están interconectadas y organizadas en diferentes capas.\n",
    "\n",
    "    Un nodo individual en una capa está conectado a muchos otros nodos de la capa anterior y de la capa siguiente. Las señales de entrada de una capa se reciben y procesan para generar la salida que se pasa a la siguiente capa.\n",
    "\n",
    "    A la primera capa de esta arquitectura a menudo se la denomina capa de entrada, la cual recibe las señales de entrada, a la última capa se la llama capa de salida, la cual produce la señal de salida y al resto de capas que están entre la capa de entrada y la capa de salida se las llama capas ocultas.\n",
    "\n",
    "Conceptos clave de las Redes Neuronales\n",
    "\n",
    "\n",
    "**A. Neurona:**\n",
    "\n",
    "Una neurona es una unidad de procesamiento de una red neuronal que está conectada a otras neuronas en la red. Esas conexiones representan las entradas y salidas de una neurona. Para cada una se estas conexiones, la neurona asigna un 'peso' (W-Weight) que es una medida de la importancia de la entrada y añade un término de sesgo (b-bias).\n",
    "\n",
    "\n",
    "\n",
    "**B. Funciones de Activación:**\n",
    "\n",
    "Las funciones de activación se usan para aplicar transformaciones no lineales en la entrada para lanzar la salida. El propósito de las funciones de activación es predecir la clase correcta de la variable objetivo basándose en las combinaciones de las variables de entrada. Algunas de las funciones de activación más populares son Relu, Sigmoide, y TanH.\n",
    "\n",
    "\n",
    "\n",
    "**C. Propagación hacia Adelante (Forward Propagation):**\n",
    "\n",
    "El modelo de red neuronal funciona con el proceso llamado propagación hacia delante en el cual se pasa la salida de activación en esa dirección sobre la red, de capa en capa.\n",
    "\n",
    "Z = W*X + b\n",
    "A = g(Z)\n",
    "\n",
    "    g es la función de activación\n",
    "    A es la activación usando la entrada\n",
    "    W es el peso asociado con la entrada\n",
    "    B es el sesgo asociado al nodo\n",
    "\n",
    "\n",
    "\n",
    "**D. Error de Computación:**\n",
    "\n",
    "La red neuronal aprende mejorando los valores de peso y sesgo. El modelo computa el error en la salida predicha en la última capa el cual es usado para hacer pequeños ajustes de los valores de los pesos y de los sesgos. Los ajustes se hacen de tal manera que el error total se minimiza. La función de pérdida, (Loss function), mide el error en la capa final y la función de coste, (Cost function), mide el error total de la red.\n",
    "\n",
    "Pérdida = Valor_Actual - Valor_Predicho\n",
    "\n",
    "Coste = Sumatoria (Pérdida)\n",
    "\n",
    "\n",
    "\n",
    "**E. Propagación hacia Atrás (Backward Propagation):**\n",
    "\n",
    "El modelo de red neuronal funciona con el proceso llamado propagación hacia atrás (backpropagation) en el cual el error se pasa hacia las capas anteriores de manera que esas capas también pueden mejorar el valor asociado a peso y sesgo. Usa el algoritmo llamado Gradiente Descente donde el error se minimiza y se obtienen los valores óptimos de los pesos y los sesgos. Este ajuste de los pesos y los sesgos se consigue computando el error de derivación, derivando los pesos, los sesgos y restandolos de los valores originales.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Implementando una red neuronal, clasificación binaria\n",
    "\n",
    "Hagamos una sencilla implementación de una red neuronal en python para clasificación binaria, para clasificar si una imagen dada es 0 o 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Preparación del dataset**\n",
    "\n",
    "El primer paso es cargar y preparar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# incluye solo las filas con etiquetas = 0 o 1 (clasificación binaria)\n",
    "X = train[train['label'].isin([0, 1])]\n",
    "\n",
    "# variable objetivo\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "\n",
    "# elimina la etiqueta de X\n",
    "X = X.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Implementando una Función de Activación**\n",
    "\n",
    "Usaremos la función de activación sigmoide porque su salida son valores entre 0 y 1 por lo que es una buena elección para un problema de clasificación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementando una sigmoide como función de activación\n",
    "def sigmoide(z):\n",
    "    s = 1.0/ (1 + np.exp(-z))    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Se define la Arquitectura de la Red Neuronal**\n",
    "\n",
    "Se crea un modelo con tres capas - Entrada, Oculta, Salida   (Input, Hidden, Output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arquitectura(X, Y):\n",
    "    # nodos de la capa de entrada\n",
    "    n_x = X.shape[0] \n",
    "    # nodos de la capa oculta\n",
    "    n_h = 10          \n",
    "    # nodos de la capa de salida\n",
    "    n_y = Y.shape[0] \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Se definen los Parámetros de la Red Neuronal** \n",
    "\n",
    "Los parámetros de una red neuronal son los pesos y los sesgos que se necesitan inicializar a cero. La primera capa solo contiene entradas por lo que no hay pesos ni sesgo, pero tanto las capas ocultas como la capa de salida tienen términos de sesgo y pesos. (W1, b1 y W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametros(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01     # inicialización aleatoria\n",
    "    b1 = np.zeros((n_h, 1))                  # inicialización a cero\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 \n",
    "    b2 = np.zeros((n_y, 1)) \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Implementando Forward Propagation**\n",
    "\n",
    "La capa oculta y la capa de salida se activarán usando la función sigmoide y pasarán la señal hacia adelante. Mientras se computa ésta activación, la entrada se multiplica por el peso y se suma el sesgo antes de pasarla a ésta función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "    A1 = sigmoide(Z1)\n",
    "\n",
    "    Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "    A2 = sigmoide(Z2)\n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.6 Computando el Error de la Red**\n",
    "\n",
    "Para computar el coste, una manera directa de hacerlo es calcular el error absoluto entre la predicción y el valor real. Pero una mejor función de pérdida es la función log-pérdida que se define como sigue :\n",
    "\n",
    "-Sumatoria ( Log (Pred) Real + Log (1 - Pred) Real ) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_error(Pred, Real):\n",
    "    logprobs = np.multiply(np.log(Pred), Real)+ np.multiply(np.log(1-Pred), 1-Real)\n",
    "    coste = -np.sum(logprobs) / Real.shape[1] \n",
    "    return np.squeeze(coste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.7 Implementando Backward Propagation**\n",
    "\n",
    "En la función de backward propagation, el error se pasa hacia atrás a las capas previas  y se calculan las derivadas de los pesos y los sesgos. Entonces se actualizan los pesos y los sesgos a través de las derivadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(params, activacion, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # capa de salida\n",
    "    dZ2 = activacion['A2'] - Y                    # calcula derivada del error \n",
    "    dW2 = np.dot(dZ2, activacion['A1'].T) / m     # calcula derivada del peso \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m    # calcula derivada del sesgo\n",
    "    \n",
    "    # capa oculta\n",
    "    dZ1 = np.dot(params['W2'].T, dZ2)*(1-np.power(activacion['A1'], 2))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims=True)/m\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1.2):\n",
    "    # alpha es la tasa de aprendizaje del modelo \n",
    "    \n",
    "    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n",
    "    params['b1'] = params['b1'] - alpha * derivatives['db1']\n",
    "    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n",
    "    params['b2'] = params['b2'] - alpha * derivatives['db2']\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2.8 Compila y Entrena el Modelo**\n",
    "\n",
    "Crea una función que compila todas las funciones clave y crea un modelo de red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, n_h, num_iterations=100):\n",
    "    n_x = arquitectura(X, Y)[0]\n",
    "    n_y = arquitectura(X, Y)[2]\n",
    "    \n",
    "    params = parametros(n_x, n_h, n_y)\n",
    "    for i in range(0, num_iterations):\n",
    "        results = forward_propagation(X, params)\n",
    "        error = n_error(results['A2'], Y)\n",
    "        derivatives = backward_propagation(params, results, X, Y) \n",
    "        params = update_parameters(params, derivatives)    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "y = Y.values.reshape(1, Y.size)\n",
    "x = X.T.as_matrix()\n",
    "model = neural_network(x, y, n_h = 10, num_iterations = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.9 Predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61143996 0.07539908 0.95175608 ... 0.95175608 0.07539908 0.95175608]\n",
      "Precisión: 97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def predice(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    print (results['A2'][0])\n",
    "    predicciones = np.around(results['A2'])    \n",
    "    return predicciones\n",
    "\n",
    "predicciones = predice(model, x)\n",
    "print ('Precisión: %d' % float((np.dot(y,predicciones.T) + np.dot(1-y,1-predicciones.T))/float(y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Implementando una red neuronal, multiclasificación \n",
    "\n",
    "En el paso previo se ha discutido sobre como implementar una red neuronal para clasificación binaria. Librerías de Python como sklearn tienen excelentes implementaciones de redes neuronales eficientes que se pueden usar directamente en un dataset. En ésta sección se implementará una red neuronal multiclase para clasificar imágenes de números desde 0 a 9.\n",
    "\n",
    "\n",
    "**3.1 Preparación del Dataset**\n",
    "\n",
    "Se separa el dataset en entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neural_network\n",
    "from sklearn import  metrics\n",
    "import tensorflow as tf                           # quitar texto de tensorflow\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "Y = train['label'][:10000]                        \n",
    "X = train.drop(['label'], axis = 1)[:10000]       \n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3.2 Entrenando el modelo**\n",
    "\n",
    "Se entrena una red neuronal con 10 capas ocultas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=18, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = neural_network.MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5,), solver='lbfgs', random_state=18)\n",
    "modelo.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Predicciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.96      0.92      0.94       210\n",
      "           2       0.12      0.99      0.22       220\n",
      "           3       0.00      0.00      0.00       190\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.00      0.00      0.00       194\n",
      "           6       0.00      0.00      0.00       190\n",
      "           7       0.00      0.00      0.00       233\n",
      "           8       0.00      0.00      0.00       197\n",
      "           9       0.00      0.00      0.00       192\n",
      "\n",
      "   micro avg       0.20      0.20      0.20      2000\n",
      "   macro avg       0.11      0.19      0.12      2000\n",
      "weighted avg       0.11      0.20      0.12      2000\n",
      ":\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prediccion = modelo.predict(x_val)\n",
    "print(\"Reporte de clasificación:\\n %s:\" % (metrics.classification_report(y_val, prediccion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-DNN-CNN\n",
    "\n",
    "Una red neuronal profunda (Deep Neural Networks) de un gran número de capas ocultas las cuales tratan de extraer características de bajo nivel de las imágenes. Algunos ejemplor de redesprofundas son las redes convolucionales y las redes neuronales recurrentes.\n",
    "\n",
    "\n",
    "**Redes Neuronales Convolucionales**\n",
    "\n",
    "En las redes convolucionales cada imagen de entrada se trata como una matriz de los valores de los pixeles que representan la cantidad de oscuridad en cada pixel. A diferencia de las redes tradicionales que tratan las imágenes como elementos de una dimensión, las redes convolucionales consideran la localización de los pixeles y sus vecinos para la clasificación.\n",
    "\n",
    "![](neural2.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Componentes clave de una red convolución**\n",
    "\n",
    "A. Capa convolucional: en ésta capa, una matriz de pesos (o kernel) se usa para extraer ceracterísticas de bajo nivel de las imágenes. El kernel con sus pesos rota sobre la matriz de la imagen con una ventana corrediza para obtener la salida convolucionada. La matriz de pesos se comporta como un filtro en una imagen extrayendo información particular de la matriz original de la imagen. Durante el proceso de convolución, los pesos se ajustan de tal manera que se minimice la función de pérdida.\n",
    "\n",
    "B. Stride: El stride se define como el número de pasos que la matriz de pesos da moviéndose por la imagen tomando N pixeles cada vez. Si la matriz de pesos se mueve N pixeles cada vez, se dice que el stride es N.\n",
    "\n",
    "![](neural3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagen desde.. - www.deeplearning.net\n",
    "\n",
    "C. Capa Pooling: Las capas de pooling se usan para extraer las características que más información tienen desde la salida convolucional generada.\n",
    "\n",
    "![](neural4.png)\n",
    "\n",
    "\n",
    "D. Capa de salida: Para generar la salida final, se aplica una capa densa o completamente conectada (fully connected layer) con una función de activación sigmoide (softmax). La función softmax se usa para generar las probabilidades de cada clase objetivo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Implementando una red neuronal convolucional CNN\n",
    "\n",
    "\n",
    "**5.1 Preparación del Dataset**\n",
    "\n",
    "En el primer paso se prepara el dataset y se separa en entrenamiento y validación. Para el modelado y el entrenamiento, se usa la librería de Python Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "Y = train['label']\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X.as_matrix(), Y.as_matrix(), test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**5.2 Definición de lo parámetros de la red**\n",
    "\n",
    "Los parámetros son :\n",
    "\n",
    "Batch    - Número de filas de los datos de entrada para usar en cada iteración para el entrenamiento. \n",
    "\n",
    "N Clases - Número de clases objetivo.\n",
    "\n",
    "Epocas   - Número de iteraciones del modelo de red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros de la red\n",
    "batch = 128\n",
    "n_clases = 10\n",
    "epocas = 5 \n",
    "\n",
    "# dimensiones de las imagenes de entrada\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**5.3 Preproceso de la entrada**\n",
    "\n",
    "En el paso de preprocesamiento, los vectores de la imagen se redimensionan a vectores de 4 dimensiones: batch total, ancho y alto de la imagen y el canal. En este caso el canal es 1, pues es una imagen en blanco y negro y no en (R,G,B). El siguiente paso es normalizar los datos de entrada dividiendo todos los valores entre 255, el máximo del valor de los pixeles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# preproceso datos entrenamiento \n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "# preproceso datos validacion\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.astype('float32')\n",
    "x_val /= 255\n",
    "\n",
    "dim_entrada = (img_rows, img_cols, 1)\n",
    "\n",
    "# convierte la variable objetivo \n",
    "y_train = keras.utils.to_categorical(y_train, n_clases)\n",
    "y_val = keras.utils.to_categorical(y_val, n_clases)\n",
    "\n",
    "# preproceso datos test\n",
    "Xtest = test.as_matrix()\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**5.4 Creación de la arquitectura del modelo CNN**\n",
    "\n",
    "En éste paso, se crea la estructura de la red con las siguientes capas:\n",
    "\n",
    "    Capa convolucional con tamaño de kernel = 3*3, 32 unidades convolutionales, y RelU como función de activación\n",
    "    \n",
    "    Capa convolucional con tamaño de kernel = 3*3, 64 unidades convolutional, y RelU como función de activación\n",
    "    \n",
    "    Capa Max Pooling Layer con tamaño de pooling = 2*2\n",
    "    \n",
    "    Capa de Dropout : Una capa de dropout se usa para regularización y reducir el sobreajuste\n",
    "    Capa Flatten (Aplana): Una capa para convertir la salida a un array de una dimensión\n",
    "    \n",
    "    Capa Densa : Una capa densa es una capa completamente conectada donde cada nodo se conecta a todos los nodos de la siguiente capa. Esta red contiene 128 neuronas pero se puede cambiar para más experimentos\n",
    "    \n",
    "    Otra capa Dropout Layer para regularización\n",
    "    \n",
    "    Capa Final de salida : Una capa densa con 10 neuronas para generar las clases de salida\n",
    "\n",
    "En la red más simple que se ha hecho en el paso 1, la función de pérdida era la Log-Loss y el algoritmo de optimización era el Gradiente Descente. En éste caso, se usará la entropía cruzada categórica (categorical_crossentropy), pues es una clasificación multiclase, como función de pérdida y Adadelta como función de optimización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo = Sequential()\n",
    "\n",
    "# añade primera capa convolucional\n",
    "modelo.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=dim_entrada))\n",
    "\n",
    "# añade segunda capa convolucional\n",
    "modelo.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# añade una capa max pooling \n",
    "modelo.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# añade una capa dropout \n",
    "modelo.add(Dropout(0.25))\n",
    "\n",
    "# añade una capa flatten \n",
    "modelo.add(Flatten())\n",
    "\n",
    "# add dense layer\n",
    "modelo.add(Dense(128, activation='relu'))\n",
    "\n",
    "# add another dropout layer\n",
    "modelo.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer\n",
    "modelo.add(Dense(n_clases, activation='softmax'))\n",
    "\n",
    "# complile the model and view its architecur\n",
    "modelo.compile(loss=keras.losses.categorical_crossentropy,  optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37800 samples, validate on 4200 samples\n",
      "Epoch 1/5\n",
      "37800/37800 [==============================] - 217s 6ms/step - loss: 0.3535 - acc: 0.8905 - val_loss: 0.1042 - val_acc: 0.9717\n",
      "Epoch 2/5\n",
      "37800/37800 [==============================] - 218s 6ms/step - loss: 0.1099 - acc: 0.9672 - val_loss: 0.0699 - val_acc: 0.9786\n",
      "Epoch 3/5\n",
      "37800/37800 [==============================] - 232s 6ms/step - loss: 0.0791 - acc: 0.9767 - val_loss: 0.0530 - val_acc: 0.9833\n",
      "Epoch 4/5\n",
      "37800/37800 [==============================] - 238s 6ms/step - loss: 0.0641 - acc: 0.9811 - val_loss: 0.0500 - val_acc: 0.9843\n",
      "Epoch 5/5\n",
      "37800/37800 [==============================] - 235s 6ms/step - loss: 0.0557 - acc: 0.9831 - val_loss: 0.0428 - val_acc: 0.9860\n",
      "Test accuracy: 0.9859523809523809\n"
     ]
    }
   ],
   "source": [
    "modelo.fit(x_train, y_train, batch_size=batch, epochs=epocas, verbose=1, validation_data=(x_val, y_val))\n",
    "precision = modelo.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test de precision:', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelo.predict(Xtest)\n",
    "y_clases = pred.argmax(axis=-1)\n",
    "res = pd.DataFrame()\n",
    "res['Imagen_Id'] = list(range(1,28001))\n",
    "res['Etiqueta'] = y_classes\n",
    "res.to_csv(\"output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a https://www.kaggle.com/kernels/svzip/4153864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
