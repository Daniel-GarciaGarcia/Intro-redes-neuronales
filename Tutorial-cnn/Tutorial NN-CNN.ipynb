{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial NN-CNN\n",
    "\n",
    "![](neural.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- ¿Qué son las redes neuronales?\n",
    "\n",
    "Las redes neuronales son un tipo de modelo de machine learning que están diseñadas para operar de manera similar a las neuronas biológicas y el sistema nervioso humano. Estos modelos se usan para reconocer patrones complejos y relaciones que existen en un dataset con etiquetas. Tienen las siguientes propiedades:\n",
    "\n",
    "    La arquitectura de un modelo de red neuronal está compuesta de un gran numero de nodos simples de procesamiento, llamados neuronas,  las cuales están interconectadas y organizadas en diferentes capas.\n",
    "\n",
    "    Un nodo individual en una capa está conectado a muchos otros nodos de la capa anterior y de la capa siguiente. The inputs form one layer are received and processed to generate the output which is passed to the next layer.\n",
    "\n",
    "    The first layer of this architecture is often named as input layer which accepts the inputs, the last layer is named as the output layer which produces the output and every other layer between input and output layer is named is hidden layers.\n",
    "\n",
    "Key concepts in a Neural Network\n",
    "\n",
    "\n",
    "A. Neuron:\n",
    "\n",
    "A Neuron is a single processing unit of a Neural Network which are connected to different other neurons in the network. These connections repersents inputs and ouputs from a neuron. To each of its connections, the neuron assigns a “weight” (W) which signifies the importance the input and adds a bias (b) term.\n",
    "\n",
    "\n",
    "\n",
    "B. Activation Functions\n",
    "\n",
    "The activation functions are used to apply non-linear transformation on input to map it to output. The aim of activation functions is to predict the right class of the target variable based on the input combination of variables. Some of the popular activation functions are Relu, Sigmoid, and TanH.\n",
    "C. Forward Propagation\n",
    "\n",
    "Neural Network model goes through the process called forward propagation in which it passes the computed activation outputs in the forward direction.\n",
    "\n",
    "Z = W*X + b\n",
    "A = g(Z)\n",
    "\n",
    "    g is the activation function\n",
    "    A is the activation using the input\n",
    "    W is the weight associated with the input\n",
    "    B is the bias associated with the node\n",
    "\n",
    "D. Error Computation:\n",
    "\n",
    "The neural network learns by improving the values of weights and bias. The model computes the error in the predicted output in the final layer which is then used to make small adjustments the weights and bias. The adjustments are made such that the total error is minimized. Loss function measures the error in the final layer and cost function measures the total error of the network.\n",
    "\n",
    "Loss = Actual_Value - Predicted_Value\n",
    "\n",
    "Cost = Summation (Loss)\n",
    "E. Backward Propagation:\n",
    "\n",
    "Neural Network model undergoes the process called backpropagation in which the error is passed to backward layers so that those layers can also improve the associated values of weights and bias. It uses the algorithm called Gradient Descent in which the error is minimized and optimal values of weights and bias are obtained. This weights and bias adjustment is done by computing the derivative of error, derivative of weights, bias and subtracting them from the original values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Implementando una red neuronal, clasificación binaria\n",
    "\n",
    "Lets implement a basic neural network in python for binary classification which is used to classify if a given image is 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Dataset Preparation\n",
    "\n",
    "First step is to load and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# include only the rows having label = 0 or 1 (binary classification)\n",
    "X = train[train['label'].isin([0, 1])]\n",
    "\n",
    "# target variable\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "\n",
    "# remove the label from X\n",
    "X = X.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Implementing a Activation Function\n",
    "\n",
    "We will use sigmoid activation function because it outputs the values between 0 and 1 so its a good choice for a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    s = 1.0/ (1 + np.exp(-z))    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Define Neural Network Architecture\n",
    "\n",
    "Create a model with three layers - Input, Hidden, Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_architecture(X, Y):\n",
    "    # nodes in input layer\n",
    "    n_x = X.shape[0] \n",
    "    # nodes in hidden layer\n",
    "    n_h = 10          \n",
    "    # nodes in output layer\n",
    "    n_y = Y.shape[0] \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Define Neural Network Parameters\n",
    "\n",
    "Neural Network parameters are weights and bias which we need to initialze with zero values. The first layer only contains inputs so there are no weights and bias, but the hidden layer and the output layer have a weight and bias term. (W1, b1 and W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_network_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01 # random initialization\n",
    "    b1 = np.zeros((n_h, 1)) # zero initialization\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01 \n",
    "    b2 = np.zeros((n_y, 1)) \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Implement Forward Propagation\n",
    "\n",
    "The hidden layer and output layer will compute the activations using sigmoid activation function and will pass it in the forward direction. While computing this activation, the input is multiplied with weight and added with bias before passing it to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.6 Compute the Network Error\n",
    "\n",
    "To compute the cost, one straight forward approach is to compute the absolute error among prediction and actual value. But a better loss function is the log loss function which is defines as :\n",
    "\n",
    "-Summ ( Log (Pred) Actual + Log (1 - Pred ) Actual ) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(Predicted, Actual):\n",
    "    logprobs = np.multiply(np.log(Predicted), Actual)+ np.multiply(np.log(1-Predicted), 1-Actual)\n",
    "    cost = -np.sum(logprobs) / Actual.shape[1] \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.7 Implement Backward Propagation\n",
    "\n",
    "In backward propagation function, the error is passed backward to previous layers and the derivatives of weights and bias are computed. The weights and bias are then updated using the derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(params, activations, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # output layer\n",
    "    dZ2 = activations['A2'] - Y # compute the error derivative \n",
    "    dW2 = np.dot(dZ2, activations['A1'].T) / m # compute the weight derivative \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m # compute the bias derivative\n",
    "    \n",
    "    # hidden layer\n",
    "    dZ1 = np.dot(params['W2'].T, dZ2)*(1-np.power(activations['A1'], 2))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims=True)/m\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1.2):\n",
    "    # alpha is the model's learning rate \n",
    "    \n",
    "    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n",
    "    params['b1'] = params['b1'] - alpha * derivatives['db1']\n",
    "    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n",
    "    params['b2'] = params['b2'] - alpha * derivatives['db2']\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.8 Compile and Train the Model\n",
    "\n",
    "Create a function which compiles all the key functions and creates a neural network model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, n_h, num_iterations=100):\n",
    "    n_x = network_architecture(X, Y)[0]\n",
    "    n_y = network_architecture(X, Y)[2]\n",
    "    \n",
    "    params = define_network_parameters(n_x, n_h, n_y)\n",
    "    for i in range(0, num_iterations):\n",
    "        results = forward_propagation(X, params)\n",
    "        error = compute_error(results['A2'], Y)\n",
    "        derivatives = backward_propagation(params, results, X, Y) \n",
    "        params = update_parameters(params, derivatives)    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "y = Y.values.reshape(1, Y.size)\n",
    "x = X.T.as_matrix()\n",
    "model = neural_network(x, y, n_h = 10, num_iterations = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80217027 0.06472329 0.89967995 ... 0.89967995 0.06472329 0.89967995]\n",
      "Accuracy: 97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def predict(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    print (results['A2'][0])\n",
    "    predictions = np.around(results['A2'])    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, x)\n",
    "print ('Accuracy: %d' % float((np.dot(y,predictions.T) + np.dot(1-y,1-predictions.T))/float(y.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Implementando una red neuronal, multiclasificación \n",
    "\n",
    "In the previous step, I discussed about how to implement a NN for binary classification in python from scratch. Python's libraries such as sklearn provides an excellent implementation of efficient neural networks which can be used to directly implement neural networks on a dataset. In this section, lets implement a multi class neural network to classify the digit shown in an image from 0 to 9\n",
    "3.1 Dataset Preparation\n",
    "\n",
    "Slice the train dataset into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neural_network\n",
    "from sklearn import  metrics\n",
    "import tensorflow as tf              # quitar texto de tensorflow\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "Y = train['label'][:10000] # use more number of rows for more training \n",
    "X = train.drop(['label'], axis = 1)[:10000] # use more number of rows for more training \n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.2 Train the Model\n",
    "\n",
    "Train a neural network model with 10 hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=18, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neural_network.MLPClassifier(alpha=1e-5, hidden_layer_sizes=(5,), solver='lbfgs', random_state=18)\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       186\n",
      "           1       0.96      0.92      0.94       210\n",
      "           2       0.12      0.99      0.22       220\n",
      "           3       0.00      0.00      0.00       190\n",
      "           4       0.00      0.00      0.00       188\n",
      "           5       0.00      0.00      0.00       194\n",
      "           6       0.00      0.00      0.00       190\n",
      "           7       0.00      0.00      0.00       233\n",
      "           8       0.00      0.00      0.00       197\n",
      "           9       0.00      0.00      0.00       192\n",
      "\n",
      "   micro avg       0.20      0.20      0.20      2000\n",
      "   macro avg       0.11      0.19      0.12      2000\n",
      "weighted avg       0.11      0.20      0.12      2000\n",
      ":\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(x_val)\n",
    "print(\"Classification Report:\\n %s:\" % (metrics.classification_report(y_val, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-DNN-CNN\n",
    "\n",
    "Deep Neural Networks are composed of complex and many number of hidden layers which tries to extract low level features from the images. Some examples of complex deep neural networks are convolutional neural networks and Recurrent Neural Networks.\n",
    "Convolutional Neural Networks\n",
    "\n",
    "In Convolutional Neural Networks, every image input is treated as a a matrix of pixel values which represents the amount of darkness at a given pixel in the image. Unlike, tradational neural networks which treats an image as a one dimentional network, CNNs considers the location of pixels and the neighbours for classification.\n",
    "\n",
    "![](neural2.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key components of Convolutional Neural Network.\n",
    "\n",
    "A. Convolutional layer: In this layer, a kernel (or weight) matrix is used to extract low level features from the images. The kernel with its weights rotates over the image matrix in a sliding window fashion in order to obtained the convolved output. The kernel matrix behaves like a filter in an image extracting particular information from the original image matrix. During the colvolution process, The weights are learnt such that the loss function is minimized.\n",
    "\n",
    "B. Stride: Stride is defined as the number of steps the kernel or the weight matrix takes while moving across the entire image moving N pixel at a time. If the weight matrix moves N pixel at a time, it is called stride of N.\n",
    "\n",
    "![](neural3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Credits - www.deeplearning.net\n",
    "\n",
    "C. Pooling Layer: Pooling layers are used to extract the most informative features from the generated convolved output.\n",
    "\n",
    "![](neural4.png)\n",
    "\n",
    "\n",
    "D. Output Layer: To generate the final output, a dense or a fully connected layer is applied with the softmax activation function. Softmax function is used to generate the probabilities for each class of the target variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Implementando una red neuronal convolucional CNN\n",
    "\n",
    "\n",
    "5.1 Dataset Preparation\n",
    "\n",
    "In the first step lets prepare the dataset and slice it into train and validation sets. For the modelling and training purpose, we can use python's library - Keras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "Y = train['label']\n",
    "X = train.drop(['label'], axis=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X.as_matrix(), Y.as_matrix(), test_size=0.10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5.2 Define the Network Parameters\n",
    "\n",
    "Network Parameters are :\n",
    "\n",
    "Batch Size - Number of rows from the input data to use it one iteratation from the training purpose\n",
    "Num Classes - Total number of possible classes in the target variable\n",
    "Epochs - Total number of iterations for which cnn model will run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters \n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5 # Further Fine Tuning can be done\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5.3 Preprocess the Inputs\n",
    "\n",
    "In the preprocessing step the corresponding image data vectors are reshaped into a 4 dimentional vector : total batch size, width of the image, height of the image, and the channel. In our case, channel = 1 as we will only use single channel instead of three channels (R,G,B). The next step is to normalize the inputs by dividing them by max pixel value ie. 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iudh/.local/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# preprocess the train data \n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "\n",
    "# preprocess the validation data\n",
    "x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n",
    "x_val = x_val.astype('float32')\n",
    "x_val /= 255\n",
    "\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# convert the target variable \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "# preprocess the test data\n",
    "Xtest = test.as_matrix()\n",
    "Xtest = Xtest.reshape(Xtest.shape[0], img_rows, img_cols, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5.4 Create the CNN Model Architecture\n",
    "\n",
    "In this step, create the convolutional neural network architecture with following layers:\n",
    "\n",
    "    Convolutional Layer with kernel size = 3*3, 32 convolutional units, and RelU activation function\n",
    "    \n",
    "    Convolutional Layer with kernel size = 3*3, 64 convolutional units, and RelU activation function\n",
    "    \n",
    "    Max Pooling Layer with pooling matrix size = 2*2\n",
    "    \n",
    "    Dropout Layer : A dropout layer is used for regularization and reducing the overfitting\n",
    "    Flatten Layer : A layer to convert the output in one dimentional array\n",
    "    \n",
    "    Dense Layer : A dense layer is a fully connected layer in which every node is connected to every other node in the previous and next layers. In our network, it contains 128 neurons but this number can be changed for further experiments.\n",
    "    \n",
    "    Another Dropout Layer for regularization\n",
    "    \n",
    "    Final output layer : A dense layer with 10 neurons for generating the output class\n",
    "\n",
    "In the simple neural network that we implemented in step 1, the loss function was LogLoss function and the optimizing Algorithm was Gradient Descent, In this neural network, we will use categorical_crossentropy as this is a multi class classification as the loss function and Adadelta as the optimizing function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# add first convolutional layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "\n",
    "# add second convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# add one max pooling layer \n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add one dropout layer\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# add flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# add another dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add dense layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# complile the model and view its architecur\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,  optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37800 samples, validate on 4200 samples\n",
      "Epoch 1/5\n",
      "37800/37800 [==============================] - 191s 5ms/step - loss: 0.3453 - acc: 0.8939 - val_loss: 0.1177 - val_acc: 0.9643\n",
      "Epoch 2/5\n",
      "37800/37800 [==============================] - 202s 5ms/step - loss: 0.1073 - acc: 0.9677 - val_loss: 0.0605 - val_acc: 0.9829\n",
      "Epoch 3/5\n",
      "37800/37800 [==============================] - 208s 6ms/step - loss: 0.0772 - acc: 0.9764 - val_loss: 0.0575 - val_acc: 0.9824\n",
      "Epoch 4/5\n",
      "37800/37800 [==============================] - 224s 6ms/step - loss: 0.0624 - acc: 0.9817 - val_loss: 0.0486 - val_acc: 0.9840\n",
      "Epoch 5/5\n",
      "37800/37800 [==============================] - 216s 6ms/step - loss: 0.0515 - acc: 0.9847 - val_loss: 0.0384 - val_acc: 0.9876\n",
      "Test accuracy: 0.9876190476190476\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test accuracy:', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(Xtest)\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "res = pd.DataFrame()\n",
    "res['ImageId'] = list(range(1,28001))\n",
    "res['Label'] = y_classes\n",
    "res.to_csv(\"output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias a https://www.kaggle.com/kernels/svzip/4153864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
