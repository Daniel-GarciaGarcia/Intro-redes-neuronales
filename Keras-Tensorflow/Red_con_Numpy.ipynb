{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tu propia red neuronal con Numpy\n",
    "\n",
    "En este notebook vamos a construir una red usando solamente numpy y nervios de acero. Será divertido.\n",
    "\n",
    "<img src=\"frankenstein.png\" style=\"width:20%\">\n",
    "\n",
    "\n",
    "* ref => https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tqdm_utils\n",
    "import download_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa modelos y datasets precargardos desde keras\n",
    "\n",
    "download_utils.link_all_keras_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra clase principal: una capa con metodos .forward() y .backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capa:\n",
    "    '''\n",
    "    Un bloque de construccion. Cada capa es capaz de realizar dos cosas:\n",
    "    \n",
    "    - Procesar la entrada para dar una salida:               salida = capa.forward(entrada)\n",
    "    \n",
    "    - Propagar los gradientes a traves de la propia capa:    grad_in = capa.backward(entrada, grad_out)\n",
    "    \n",
    "    Algunas capas tambien tienen parametros de aprendizaje que se actualizan durante el metodo capa.backward.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        \"\"\"Aqui se pueden inicializar parametros de la capa, si hay alguno, y cosas auxiliares.\"\"\"\n",
    "        # Una capa vacia no hace nada\n",
    "        self.pesos = np.zeros(shape=(input.shape[1], 10))\n",
    "        sesgos = np.zeros(shape=(10,))\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):  # propagacion hacia adelante de los datos\n",
    "        '''\n",
    "        Toma la dimension de los datos de entrada [muestra, unidades_entrada], \n",
    "        devuelve datos de salida [muestra, unidades_salida]\n",
    "        '''\n",
    "        # Una capa vacia solo devuelve la entrada.\n",
    "        output = np.matmul(input, self.pesos) + sesgos  # multiplicacion matricial\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):  # propagacion hacia atras del error\n",
    "        '''\n",
    "        Realiza la propagacion hacia atras a traves de la capa, con respecto a la entrada dada.\n",
    "        \n",
    "        Se calculan los gradientes de la perdida de entrada, se necesita aplicar la regla de la cadena:\n",
    "        \n",
    "        d perdida / d x  = (d perdida / d capa) * (d capa / d x)\n",
    "        \n",
    "        Afortunadamente, se recibe d perdida / d capa como entrada, \n",
    "        asi que solo se necesita multiplicar por d capa / d x. (derivadas)\n",
    "        \n",
    "        Si na capa tiene parametro,por ejemplo una capa densa (full connected, dense),\n",
    "        tambien se necesita actualizarla usando d perdida / d capa\n",
    "        '''\n",
    "        # El gradiente de una capa vacia es precisamente grad_output, pero se escribe explicitamente:\n",
    "        num_unidades = input.shape[1]\n",
    "        \n",
    "        d_capa_d_entrada = np.eye(num_unidades)\n",
    "        \n",
    "        return np.dot(grad_output, d_capa_d_entrada) # regla de la cadena, producto escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camino por delante\n",
    "\n",
    "Vamos a construir una red neuronal que clasifique números de la base de datos MNIST. Para hacerlo, necesitamos construir algunos bloques:\n",
    "\n",
    "- Capa densa - capa fully-connected ,  $f(X)=W \\cdot X + \\vec{b}$\n",
    "- Capa ReLU (unidad de rectificado lineal o otra capa no lineal que se prefiera)\n",
    "- Funcion de perdida (entropia cruzada) (loss function - crossentropy)\n",
    "- Algoritmo de propagacion hacia atras (Backprop algorithm), gradiente descente estocastico\n",
    "\n",
    "Vamos a verlos uno por uno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa no lineal\n",
    "\n",
    "Esta es la capa más simple que se puede tener: simplemente aplica una función no lineal a cada elemento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Capa):\n",
    "    def __init__(self):\n",
    "        '''ReLU aplica un rectificado lineal a todas las entradas'''\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''Aplica ReLU elemento a elemento a la matriz [muestra, unidades_entrada]'''\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        '''Calcula el gradiente de la perdida para las entradas de ReLU'''\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algunas pruebas\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
    "num_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, num_grads, rtol=1e-3, atol=0),\\\n",
    "    'el gradiente devuelto por la capa no coincide con el gradiente calculado numericamente' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En primer lugar: funciones lambda \n",
    "\n",
    "En python, se pueden definir funciones en una sola linea usando la funcion `lambda`,\n",
    "\n",
    "que tiene una sintaxis tal que: `lambda param1, param2: expression`\n",
    "\n",
    "\n",
    "Por ejemplo: `f = lambda x, y: x+y` es equivalente a la funcion:\n",
    "\n",
    "```\n",
    "def f(x,y):\n",
    "    return x+y\n",
    "```\n",
    "Para mas informacion,  [aqui](http://www.secnetix.de/olli/Python/lambda_functions.hawk).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa densa (full connected)\n",
    "\n",
    "Vamos a construir algo mas complicado. A diferencia de la no linealidad, una capa densa realmente aprende algo.\n",
    "\n",
    "Una capa densa aplica una transformacion afin. En forma vectorial se puede describir:\n",
    "\n",
    "$$f(X)= W \\cdot X + \\vec b $$\n",
    "\n",
    "Donde \n",
    "* X es una matriz de caracteristicas de dimensiones [tamaño_muestra, num_caracteristicas],\n",
    "* W es una matriz de pesos [num_caracteristicas, num_salidas] \n",
    "* b es un vector de sesgos [num_salidas].\n",
    "\n",
    "Tanto W como b se inicializan durante la creacion de la capa y se actualizan cada vez que se llama al metodo backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Densa(Capa):\n",
    "    def __init__(self, in_unids, out_unids, tasa=0.1):\n",
    "        '''\n",
    "        Una capa densa es una capa que realiza una transformacion afin:\n",
    "        f(x) = <W*x> + b\n",
    "        tasa es la tasa de aprendizaje de la red (learning rate)\n",
    "        '''\n",
    "        self.tasa = tasa\n",
    "        \n",
    "        # inicializa los pesos con numeros pequeños aleatorios. Se usa un inicio normal, \n",
    "        # pero hay mejores maneras de hacerlo. Una vez que te funcione, prueba esto: http://bit.ly/2vTlmaJ\n",
    "        self.pesos = np.random.randn(in_unids, out_unids)*0.01\n",
    "        self.sesgos = np.zeros(out_unids)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        '''\n",
    "        Realiza la transformacion afin:\n",
    "        f(x) = <W*x> + b\n",
    "        \n",
    "        dimension de entrada: [muestra, unidades_entrada]\n",
    "        dimension de salida: [muestra, unidades_salida]\n",
    "        '''\n",
    "        return np.matmul(input, self.pesos) + self.sesgos\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \n",
    "        # se calcula d f / d x = d f / d densa * d densa / d x\n",
    "        # donde d densa/ d x = matriz de pesos traspuesta\n",
    "        grad_input = np.dot(grad_output,np.transpose(self.pesos))\n",
    "        \n",
    "        # se calcula el gradiente de los pesos y los sesgos\n",
    "        grad_pesos = np.transpose(np.dot(np.transpose(grad_output),input))\n",
    "        grad_sesgos = np.sum(grad_output, axis = 0)\n",
    "        \n",
    "        assert grad_pesos.shape == self.pesos.shape and grad_sesgos.shape == self.sesgos.shape\n",
    "        # Aqui se realiza el paso de gradiente descente estocastico. \n",
    "        # Despues, puedes intentar mejorarlo.\n",
    "        self.pesos = self.pesos - self.tasa * grad_pesos\n",
    "        self.sesgos = self.sesgos - self.tasa * grad_sesgos\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeando la capa densa\n",
    "\n",
    "Se haran algunos tests para asegurarnos que la capa densa funciona adecuadamente. Si funciona, se printea ¡Bien hecho!\n",
    "\n",
    "... o no, y tendras que arreglar algo. Para esto alguno consejos:\n",
    "* Asegurate de calcular los gradientes para pesos y sesgo como __la suma de los gradientes sobre la muestra__. La salida ya está dividida entre el tamaño de la muestra.\n",
    "* Si estás debbugueando, procura guardar los gradientes en una clase, como \"self.grad_w = grad_w\" o printea los primeros pesos. Esto ayuda.\n",
    "* Si nada ayuda, trata de ignorar el test y pasa a la fase de entrenamiento. Si entrena, está pasando algo que no afecta al funcionamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bien hecho!\n"
     ]
    }
   ],
   "source": [
    "l = Densa(128, 150)\n",
    "\n",
    "assert -0.05 < l.pesos.mean() < 0.05 and 1e-3 < l.pesos.std() < 1e-1,\\\n",
    "    'Los pesos iniciales deben tener media cero y poca varianza.'\\\n",
    "    'Si sabes lo que haces, borra este assert.'\n",
    "\n",
    "assert -0.05<l.sesgos.mean()<0.05, 'Los sesgos deben tener media cero. Ignora esto si por alguna razón pones otra cosa'\n",
    "\n",
    "\n",
    "# Para testear las salidas, explicitamente se ponen valores fijos. ¡NO HACER ESTO CON LA RED REAL!\n",
    "l = Densa(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.pesos = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.sesgos = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print('¡Bien hecho!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bien hecho!\n"
     ]
    }
   ],
   "source": [
    "# Para testear los gradientess, usamos los gradientes obtenidos por diferencias finitas\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Densa(32, 64, tasa=0)\n",
    "\n",
    "num_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads, num_grads, rtol=1e-3, atol=0), \\\n",
    "                   'el gradiente de entrada no cuadra con el gradiente numerico'\n",
    "print('¡Bien hecho!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bien hecho!\n"
     ]
    }
   ],
   "source": [
    "#testeo de gradientes con parametros\n",
    "\n",
    "def salida_dados_ps(p,s):\n",
    "    capa = Densa(32, 64, tasa=1)\n",
    "    capa.pesos = np.array(p)\n",
    "    capa.sesgos = np.array(s)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return capa.forward(x)\n",
    "    \n",
    "def grad_por_params(p,s):\n",
    "    capa = Densa(32, 64, tasa=1)\n",
    "    capa.pesos = np.array(p)\n",
    "    capa.sesgos = np.array(s)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    capa.backward(x,np.ones([10,64]) / 10.)\n",
    "    return  p-capa.pesos, s-capa.sesgos\n",
    "    \n",
    "p,s = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "num_dp = eval_numerical_gradient(lambda p: salida_dados_ps(p,s).mean(0).sum(),p)\n",
    "num_ds = eval_numerical_gradient(lambda s: salida_dados_ps(p,s).mean(0).sum(),s)\n",
    "\n",
    "grad_p, grad_s = grad_por_params(p,s)\n",
    "\n",
    "assert np.allclose(num_dp, grad_p, rtol=1e-3, atol=0), 'los pesos del gradiente no concuerdan con el valor numerico'\n",
    "assert np.allclose(num_ds, grad_s, rtol=1e-3, atol=0), 'los pesos del gradiente no concuerdan con el valor numerico'\n",
    "print('¡Bien hecho!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La funcion de perdida (loss function)\n",
    "\n",
    "Dado que queremos predecir probabilidades, seria logico definir la funcion softmax en la red y computar la funcion de perdida dadas las probabilidades predichas. Sin embargo, existe una manera mejor de hacer esto.\n",
    "\n",
    "Escribiendo la expresion de la entropia cruzada como funcion de los logits de la funcion softmax, se veria algo como:\n",
    "\n",
    "$$ perdida = - log \\space {e^{a_{correcto}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
    "\n",
    "Mirando mas profundamente, se puede reescribir como:\n",
    "\n",
    "$$ perdida = - a_{correcto} + log {\\underset i \\sum e^{a_i} } $$\n",
    "\n",
    "A esto se le llama Log-softmax y es mejor que hacer directamente log(softmax(a)) en todos los aspectos:\n",
    "* Mejor estabilidad numerica\n",
    "* Mas facil de derivar\n",
    "* Computacion mas rapida\n",
    "\n",
    "\n",
    "Usaremos log-softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits,respuestas):\n",
    "    '''Calcula la entropia cruzada desde logits[muestra,n_clases] y las ids de la respuesta correcta'''\n",
    "    logits_respuestas = logits[np.arange(len(logits)), respuestas]\n",
    "    \n",
    "    x_entropia = -logits_respuestas + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return x_entropia\n",
    "\n",
    "def grad_softmax(logits,respuestas):\n",
    "    '''\n",
    "    Calcula los gradientes de la entropia cruzada desde logits[muestra,n_clases] \n",
    "    y las ids de la respuesta correcta\n",
    "    '''\n",
    "    unos_respuestas = np.zeros_like(logits)\n",
    "    unos_respuestas[np.arange(len(logits)),respuestas] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (-unos_respuestas + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "respuestas = np.arange(50)%10\n",
    "\n",
    "softmax(logits,respuestas)\n",
    "grads = grad_softmax(logits,respuestas)\n",
    "num_grads = eval_numerical_gradient(lambda x: softmax(x, respuestas).mean(),logits)\n",
    "\n",
    "assert np.allclose(num_grads, grads, rtol=1e-3, atol=0), 'La implementacion ha fallado.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red completa\n",
    "\n",
    "Ahora vamos a combinar todo para construir una red neuronal funcional. Se usara para clasificar numeros escritos a mano, asi que lo primero cargar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # quita avisos\n",
    "\n",
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos la red como una lista de capas, cada aplicada a la salida de la anterior. Con esta configuracion, calcular las predicciones se vuelve trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = []\n",
    "red.append(Densa(X_train.shape[1],100))\n",
    "red.append(ReLU())\n",
    "red.append(Densa(100,200))\n",
    "red.append(ReLU())\n",
    "red.append(Densa(200,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(red, X):\n",
    "    '''\n",
    "    Calcula las activaciones de todas las capas de la red aplicandolas secuencialmente.\n",
    "    Devuelve una lista de activaciones para cada capa. \n",
    "    Asegurate que las ultimas activaciones corresponden a los logits.\n",
    "    '''\n",
    "    activaciones = []\n",
    "    input = X\n",
    "\n",
    "    for i in range(len(red)):\n",
    "        activaciones.append(red[i].forward(X))\n",
    "        X = red[i].forward(X)\n",
    "        \n",
    "    assert len(activaciones) == len(red)\n",
    "    return activaciones\n",
    "\n",
    "\n",
    "\n",
    "def predice(red, X):\n",
    "    '''\n",
    "    Calcula las predicciomnes de la red.\n",
    "    '''\n",
    "    logits = forward(red, X)[-1]\n",
    "    return logits.argmax(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "def train(red, X, y):\n",
    "    '''\n",
    "    Entrena la red en una muestra dada de X e y.\n",
    "    Primero forward para tener todas las activaciones.\n",
    "    Luego capa.backward desde la ultima a la primera, una vez que se tiene el backward\n",
    "    para todas las capas, ya se ha hecho un paso del gradiente.\n",
    "    '''\n",
    "    \n",
    "    # Se obtienen las activaciones\n",
    "    activaciones = forward(red, X)\n",
    "    #entradas_capa = [X]+activaciones  #entradas_capa[i] es una entrada par red[i]\n",
    "    logits = activaciones[-1]\n",
    "    \n",
    "    # Se calcula la perdida y el gradiente inicial\n",
    "    perdida = softmax(logits, y)\n",
    "    grad_perdida = grad_softmax(logits, y)\n",
    "    \n",
    "    for i in range(1, len(red)):\n",
    "        grad_perdida = red[len(red) - i].backward(activaciones[len(red) - i - 1], grad_perdida)\n",
    "        \n",
    "    return np.mean(perdida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vez de testear, hacemos el print del acierto de entrenamiento y validacion en cada epoca.\n",
    "\n",
    "Si la implementacion es correcta, el acierto deberia estar entre el 90~93% a >97% con la red por defecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucle de entrenamiento\n",
    "\n",
    "Como es usual, se separa en pequeñas muestras para alimentar la red y actualizar los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muestras(entradas, objetivos, batch, shuffle=False):\n",
    "    assert len(entradas) == len(objetivos)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(entradas))\n",
    "    for idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(entradas) - batch + 1, batch)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[idx:idx + batch]\n",
    "        else:\n",
    "            excerpt = slice(idx, idx + batch)\n",
    "        yield entradas[excerpt], objetivos[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca 24\n",
      "Acierto entrenamiento: 0.89598\n",
      "Acierto validacion: 0.9045\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoca in range(25):\n",
    "\n",
    "    for x_batch, y_batch in muestras(X_train, y_train, batch=32,shuffle=True):\n",
    "        train(red, x_batch, y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predice(red, X_train) == y_train))\n",
    "    val_log.append(np.mean(predice(red, X_val) == y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print('Epoca',epoca)\n",
    "    print('Acierto entrenamiento:',train_log[-1])\n",
    "    print('Acierto validacion:',val_log[-1])\n",
    "    plt.plot(train_log,label='acierto train')\n",
    "    plt.plot(val_log,label='acierto val')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea de revision\n",
    "\n",
    "\n",
    "#### Opcion I: inicializacion\n",
    "* Implementar una capa Densa con inicializacion de Xavier explicada [aqui](http://bit.ly/2vTlmaJ)\n",
    "\n",
    "Para realizar esta tarea, se debe hacer un experimento mostrando como funciona la inicializacion de Xavier en comparacion con la inicializacion por defecto en redes profundas (5+ capas).\n",
    "\n",
    "\n",
    "#### Opcion II: regularizacion\n",
    "* Implementar una version de la capa Densa con regularizacion L2: cuando se actualizan los pesos de una capa, se ajusta el gradiente a minimizar\n",
    "\n",
    "$$ Perdida = Entropia_cruzada + \\alpha \\cdot \\underset i \\sum {w_i}^2 $$\n",
    "\n",
    "Para realizar esta tarea, se debe hacer un experimento mostrando si la regularizacion mitiga el sobreajuste en el caso de tener un gran numero de neuronas. Considerar ajustar $\\alpha$ para mejores resultados.\n",
    "\n",
    "#### Opcion III: optimizacion\n",
    "* Implementar una version de la capa Densa que use momentum/rmsprop o cualquier metodo que haya funcionado mejor la ultima vez.\n",
    "\n",
    "Most of those methods require persistent parameters like momentum direction or moving average grad norm, but you can easily store those params inside your layers.\n",
    "\n",
    "To pass this assignment, you must conduct an experiment showing how your chosen method performs compared to vanilla SGD.\n",
    "\n",
    "### General remarks\n",
    "_Please read the peer-review guidelines before starting this part of the assignment._\n",
    "\n",
    "In short, a good solution is one that:\n",
    "* is based on this notebook\n",
    "* runs in the default course environment with Run All\n",
    "* its code doesn't cause spontaneous eye bleeding\n",
    "* its report is easy to read.\n",
    "\n",
    "_Formally we can't ban you from writing boring reports, but if you bored your reviewer to death, there's noone left alive to give you the grade you want._\n",
    "\n",
    "\n",
    "### Bonus assignments\n",
    "\n",
    "As a bonus assignment (no points, just swag), consider implementing Batch Normalization ([guide](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)) or Dropout ([guide](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)). Note, however, that those \"layers\" behave differently when training and when predicting on test set.\n",
    "\n",
    "* Dropout:\n",
    "  * During training: drop units randomly with probability __p__ and multiply everything by __1/(1-p)__\n",
    "  * During final predicton: do nothing; pretend there's no dropout\n",
    "  \n",
    "* Batch normalization\n",
    "  * During training, it substracts mean-over-batch and divides by std-over-batch and updates mean and variance.\n",
    "  * During final prediction, it uses accumulated mean and variance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
