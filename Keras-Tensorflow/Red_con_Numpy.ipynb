{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tu propia red neuronal con Numpy\n",
    "\n",
    "En este notebook vamos a construir una red usando solamente numpy y nervios de acero. Será divertido.\n",
    "\n",
    "<img src=\"frankenstein.png\" style=\"width:20%\">\n",
    "\n",
    "\n",
    "* ref => https://towardsdatascience.com/building-an-artificial-neural-network-using-pure-numpy-3fe21acc5815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import tqdm_utils\n",
    "import download_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa modelos y datasets precargardos desde keras\n",
    "\n",
    "download_utils.link_all_keras_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra clase principal: una capa con metodos .forward() y .backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capa:\n",
    "    \"\"\"\n",
    "    Un bloque de construccion. Cada capa es capaz de realizar dos cosas:\n",
    "    \n",
    "    - Procesar la entrada para dar una salida:               salida = capa.forward(entrada)\n",
    "    \n",
    "    - Propagar los gradientes a traves de la propia capa:    grad_in = capa.backward(entrada, grad_out)\n",
    "    \n",
    "    Algunas capas tambien tienen parametros de aprendizaje que se actualizan durante el metodo capa.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Aqui se pueden inicializar parametros de la capa, si hay alguno, y cosas auxiliares.\"\"\"\n",
    "        # Una capa vacia no hace nada\n",
    "        self.pesos = np.zeros(shape=(input.shape[1], 10))\n",
    "        sesgos = np.zeros(shape=(10,))\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):  # propagacion hacia adelante de los datos\n",
    "        \"\"\"\n",
    "        Toma la dimension de los datos de entrada [muestra, unidades_entrada], \n",
    "        devuelve datos de salida [muestra, unidades_salida]\n",
    "        \"\"\"\n",
    "        # Una capa vacia solo devuelve la entrada.\n",
    "        output = np.matmul(input, self.pesos) + sesgos  # multiplicacion matricial\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):  # propagacion hacia atras del error\n",
    "        \"\"\"\n",
    "        Realiza la propagacion hacia atras a traves de la capa, con respecto a la entrada dada.\n",
    "        \n",
    "        Se calculan los gradientes de la perdida de entrada, se necesita aplicar la regla de la cadena:\n",
    "        \n",
    "        d perdida / d x  = (d perdida / d capa) * (d capa / d x)\n",
    "        \n",
    "        Afortunadamente, se recibe d perdida / d capa como entrada, \n",
    "        asi que solo se necesita multiplicar por d capa / d x. (derivadas)\n",
    "        \n",
    "        Si na capa tiene parametro,por ejemplo una capa densa (full connected, dense),\n",
    "        tambien se necesita actualizarla usando d perdida / d capa\n",
    "        \"\"\"\n",
    "        # El gradiente de una capa vacia es precisamente grad_output, pero se escribe explicitamente:\n",
    "        num_unidades = input.shape[1]\n",
    "        \n",
    "        d_capa_d_entrada = np.eye(num_unidades)\n",
    "        \n",
    "        return np.dot(grad_output, d_capa_d_entrada) # regla de la cadena, producto escalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camino por delante\n",
    "\n",
    "Vamos a construir una red neuronal que clasifique números de la base de datos MNIST. Para hacerlo, necesitamos construir algunos bloques:\n",
    "\n",
    "- Capa densa - capa fully-connected ,  $f(X)=W \\cdot X + \\vec{b}$\n",
    "- Capa ReLU (unidad de rectificado lineal o otra capa no lineal que se prefiera)\n",
    "- Funcion de perdida (entropia cruzada) (loss function - crossentropy)\n",
    "- Algoritmo de propagacion hacia atras (Backprop algorithm), gradiente descente estocastico\n",
    "\n",
    "Vamos a verlos uno por uno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa no lineal\n",
    "\n",
    "Esta es la capa más simple que se puede tener: simplemente aplica una función no lineal a cada elemento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Capa):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU aplica un rectificado lineal a todas las entradas\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Aplica ReLU elemento a elemento a la matriz [muestra, unidades_entrada]\"\"\"\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Calcula el gradiente de la perdida para las entradas de ReLU\"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algunas pruebas\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
    "num_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, num_grads, rtol=1e-3, atol=0),\\\n",
    "    'el gradiente devuelto por la capa no coincide con el gradiente calculado numericamente' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En primer lugar: funciones lambda \n",
    "\n",
    "En python, se pueden definir funciones en una sola linea usando la funcion `lambda`,\n",
    "\n",
    "que tiene una sintaxis tal que: `lambda param1, param2: expression`\n",
    "\n",
    "\n",
    "Por ejemplo: `f = lambda x, y: x+y` es equivalente a la funcion:\n",
    "\n",
    "```\n",
    "def f(x,y):\n",
    "    return x+y\n",
    "```\n",
    "Para mas informacion,  [aqui](http://www.secnetix.de/olli/Python/lambda_functions.hawk).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa densa (full connected)\n",
    "\n",
    "Vamos a construir algo mas complicado. A diferencia de la no linealidad, una capa densa realmente aprende algo.\n",
    "\n",
    "Una capa densa aplica una transformacion afin. En forma vectorial se puede describir:\n",
    "\n",
    "$$f(X)= W \\cdot X + \\vec b $$\n",
    "\n",
    "Donde \n",
    "* X es una matriz de caracteristicas de dimensiones [tamaño_muestra, num_caracteristicas],\n",
    "* W es una matriz de pesos [num_caracteristicas, num_salidas] \n",
    "* b es un vector de sesgos [num_salidas].\n",
    "\n",
    "Tanto W como b se inicializan durante la creacion de la capa y se actualizan cada vez que se llama al metodo backward()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Densa(Capa):\n",
    "    def __init__(self, in_unids, out_unids, tasa=0.1):\n",
    "        \"\"\"\n",
    "        Una capa densa es una capa que realiza una transformacion afin:\n",
    "        f(x) = <W*x> + b\n",
    "        tasa es la tasa de aprendizaje de la red (learning rate)\n",
    "        \"\"\"\n",
    "        self.tasa = tasa\n",
    "        \n",
    "        # inicializa los pesos con numeros pequeños aleatorios. Se usa un inicio normal, \n",
    "        # pero hay mejores maneras de hacerlo. Una vez que te funcione, prueba esto: http://bit.ly/2vTlmaJ\n",
    "        self.pesos = np.random.randn(in_unids, out_unids)*0.01\n",
    "        self.sesgos = np.zeros(out_unids)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Realiza la transformacion afin:\n",
    "        f(x) = <W*x> + b\n",
    "        \n",
    "        dimension de entrada: [muestra, unidades_entrada]\n",
    "        dimension de salida: [muestra, unidades_salida]\n",
    "        \"\"\"\n",
    "        return np.matmul(input, self.pesos) + self.sesgos\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        \n",
    "        # se calcula d f / d x = d f / d densa * d densa / d x\n",
    "        # donde d densa/ d x = matriz de pesos traspuesta\n",
    "        grad_input = np.dot(grad_output,np.transpose(self.pesos))\n",
    "        \n",
    "        # se calcula el gradiente de los pesos y los sesgos\n",
    "        grad_pesos = np.transpose(np.dot(np.transpose(grad_output),input))\n",
    "        grad_sesgos = np.sum(grad_output, axis = 0)\n",
    "        \n",
    "        assert grad_pesos.shape == self.pesos.shape and grad_sesgos.shape == self.sesgos.shape\n",
    "        # Aqui se realiza el paso de gradiente descente estocastico. \n",
    "        # Despues, puedes intentar mejorarlo.\n",
    "        self.pesos = self.pesos - self.tasa * grad_pesos\n",
    "        self.sesgos = self.sesgos - self.tasa * grad_sesgos\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testeando la capa densa\n",
    "\n",
    "Se haran algunos tests para asegurarnos que la capa densa funciona adecuadamente. Si funciona, se printea ¡Bien hecho!\n",
    "\n",
    "... o no, y tendras que arreglar algo. Para esto alguno consejos:\n",
    "* Asegurate de calcular los gradientes para pesos y sesgo como __la suma de los gradientes sobre la muestra__. La salida ya está dividida entre el tamaño de la muestra.\n",
    "* Si estás debbugueando, procura guardar los gradientes en una clase, como \"self.grad_w = grad_w\" o printea los primeros pesos. Esto ayuda.\n",
    "* Si nada ayuda, trata de ignorar el test y pasa a la fase de entrenamiento. Si entrena, está pasando algo que no afecta al funcionamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bien hecho!\n"
     ]
    }
   ],
   "source": [
    "l = Densa(128, 150)\n",
    "\n",
    "assert -0.05 < l.pesos.mean() < 0.05 and 1e-3 < l.pesos.std() < 1e-1,\\\n",
    "    'Los pesos iniciales deben tener media cero y poca varianza.'\\\n",
    "    'Si sabes lo que haces, borra este assert.'\n",
    "\n",
    "assert -0.05<l.sesgos.mean()<0.05, 'Los sesgos deben tener media cero. Ignora esto si por alguna razón pones otra cosa'\n",
    "\n",
    "\n",
    "# Para testear las salidas, explicitamente se ponen valores fijos. ¡NO HACER ESTO CON LA RED REAL!\n",
    "l = Densa(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.pesos = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.sesgos = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print('¡Bien hecho!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bien hecho!\n"
     ]
    }
   ],
   "source": [
    "# Para testear los gradientess, usamos los gradientes obtenidos por diferencias finitas\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Densa(32, 64, tasa=0)\n",
    "\n",
    "num_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads, num_grads, rtol=1e-3, atol=0), \\\n",
    "                   'el gradiente de entrada no cuadra con el gradiente numerico'\n",
    "print('¡Bien hecho!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Bien hecho!\n"
     ]
    }
   ],
   "source": [
    "#testeo de gradientes con parametros\n",
    "\n",
    "def salida_dados_ps(p,s):\n",
    "    capa = Densa(32, 64, tasa=1)\n",
    "    capa.pesos = np.array(p)\n",
    "    capa.sesgos = np.array(s)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return capa.forward(x)\n",
    "    \n",
    "def grad_por_params(p,s):\n",
    "    capa = Densa(32, 64, tasa=1)\n",
    "    capa.pesos = np.array(p)\n",
    "    capa.sesgos = np.array(s)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    capa.backward(x,np.ones([10,64]) / 10.)\n",
    "    return  p-capa.pesos, s-capa.sesgos\n",
    "    \n",
    "p,s = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "num_dp = eval_numerical_gradient(lambda p: salida_dados_ps(p,s).mean(0).sum(),p)\n",
    "num_ds = eval_numerical_gradient(lambda s: salida_dados_ps(p,s).mean(0).sum(),s)\n",
    "\n",
    "grad_p, grad_s = grad_por_params(p,s)\n",
    "\n",
    "assert np.allclose(num_dp, grad_p, rtol=1e-3, atol=0), 'los pesos del gradiente no concuerdan con el valor numerico'\n",
    "assert np.allclose(num_ds, grad_s, rtol=1e-3, atol=0), 'los pesos del gradiente no concuerdan con el valor numerico'\n",
    "print('¡Bien hecho!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La funcion de perdida (loss function)\n",
    "\n",
    "Dado que queremos predecir probabilidades, seria logico definir la funcion softmax en la red y computar la funcion de perdida dadas las probabilidades predichas. Sin embargo, existe una manera mejor de hacer esto.\n",
    "\n",
    "Escribiendo la expresion de la entropia cruzada como funcion de los logits de la funcion softmax, se veria algo como:\n",
    "\n",
    "$$ perdida = - log \\space {e^{a_{correcto}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
    "\n",
    "Mirando mas profundamente, se puede reescribir como:\n",
    "\n",
    "$$ perdida = - a_{correcto} + log {\\underset i \\sum e^{a_i} } $$\n",
    "\n",
    "A esto se le llama Log-softmax y es mejor que hacer directamente log(softmax(a)) en todos los aspectos:\n",
    "* Mejor estabilidad numerica\n",
    "* Mas facil de derivar\n",
    "* Computacion mas rapida\n",
    "\n",
    "\n",
    "Usaremos log-softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits,respuestas):\n",
    "    '''Calcula la entropia cruzada desde logits[muestra,n_clases] y las ids de la respuesta correcta'''\n",
    "    logits_respuestas = logits[np.arange(len(logits)), respuestas]\n",
    "    \n",
    "    x_entropia = -logits_respuestas + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return x_entropia\n",
    "\n",
    "def grad_softmax(logits,respuestas):\n",
    "    '''Calcula los gradientes de la entropia cruzada desde logits[muestra,n_clases] y las ids de la respuesta correcta'''\n",
    "    unos_respuestas = np.zeros_like(logits)\n",
    "    unos_respuestas[np.arange(len(logits)),respuestas] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (-unos_respuestas + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "respuestas = np.arange(50)%10\n",
    "\n",
    "softmax(logits,respuestas)\n",
    "grads = grad_softmax(logits,respuestas)\n",
    "num_grads = eval_numerical_gradient(lambda x: softmax(x, respuestas).mean(),logits)\n",
    "\n",
    "assert np.allclose(num_grads, grads, rtol=1e-3, atol=0), 'La implementacion ha fallado.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red completa\n",
    "\n",
    "Ahora vamos a combinar todo para construir una red neuronal funcional. Se usara para clasificar numeros escritos a mano, asi que lo primero cargar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # quita avisos\n",
    "\n",
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define network as a list of layers, each applied on top of previous one. In this setting, computing predictions and training becomes trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(Densa(X_train.shape[1],100))\n",
    "network.append(ReLU())\n",
    "network.append(Densa(100,200))\n",
    "network.append(ReLU())\n",
    "network.append(Densa(200,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer. \n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    input = X\n",
    "\n",
    "    for i in range(len(network)):\n",
    "        activations.append(network[i].forward(X))\n",
    "        X = network[i].forward(X)\n",
    "        \n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "def predict(network,X):\n",
    "    \"\"\"\n",
    "    Compute network predictions.\n",
    "    \"\"\"\n",
    "    logits = forward(network,X)[-1]\n",
    "    return logits.argmax(axis=-1)\n",
    "\n",
    "def train(network,X,y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    Then you can run layer.backward going from last to first layer.\n",
    "    \n",
    "    After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X]+layer_activations  #layer_input[i] is an input for network[i]\n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    for i in range(1, len(network)):\n",
    "        loss_grad = network[len(network) - i].backward(layer_activations[len(network) - i - 1], loss_grad)\n",
    "        \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of tests, we provide you with a training loop that prints training and validation accuracies on every epoch.\n",
    "\n",
    "If your implementation of forward and backward are correct, your accuracy should grow from 90~93% to >97% with the default network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "As usual, we split data into minibatches, feed each such minibatch into the network and update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24\n",
      "Train accuracy: 0.89598\n",
      "Val accuracy: 0.9045\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer-reviewed assignment\n",
    "\n",
    "Congradulations, you managed to get this far! There is just one quest left undone, and this time you'll get to choose what to do.\n",
    "\n",
    "\n",
    "#### Option I: initialization\n",
    "* Implement Dense layer with Xavier initialization as explained [here](http://bit.ly/2vTlmaJ)\n",
    "\n",
    "To pass this assignment, you must conduct an experiment showing how xavier initialization compares to default initialization on deep networks (5+ layers).\n",
    "\n",
    "\n",
    "#### Option II: regularization\n",
    "* Implement a version of Dense layer with L2 regularization penalty: when updating Dense Layer weights, adjust gradients to minimize\n",
    "\n",
    "$$ Loss = Crossentropy + \\alpha \\cdot \\underset i \\sum {w_i}^2 $$\n",
    "\n",
    "To pass this assignment, you must conduct an experiment showing if regularization mitigates overfitting in case of abundantly large number of neurons. Consider tuning $\\alpha$ for better results.\n",
    "\n",
    "#### Option III: optimization\n",
    "* Implement a version of Dense layer that uses momentum/rmsprop or whatever method worked best for you last time.\n",
    "\n",
    "Most of those methods require persistent parameters like momentum direction or moving average grad norm, but you can easily store those params inside your layers.\n",
    "\n",
    "To pass this assignment, you must conduct an experiment showing how your chosen method performs compared to vanilla SGD.\n",
    "\n",
    "### General remarks\n",
    "_Please read the peer-review guidelines before starting this part of the assignment._\n",
    "\n",
    "In short, a good solution is one that:\n",
    "* is based on this notebook\n",
    "* runs in the default course environment with Run All\n",
    "* its code doesn't cause spontaneous eye bleeding\n",
    "* its report is easy to read.\n",
    "\n",
    "_Formally we can't ban you from writing boring reports, but if you bored your reviewer to death, there's noone left alive to give you the grade you want._\n",
    "\n",
    "\n",
    "### Bonus assignments\n",
    "\n",
    "As a bonus assignment (no points, just swag), consider implementing Batch Normalization ([guide](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)) or Dropout ([guide](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)). Note, however, that those \"layers\" behave differently when training and when predicting on test set.\n",
    "\n",
    "* Dropout:\n",
    "  * During training: drop units randomly with probability __p__ and multiply everything by __1/(1-p)__\n",
    "  * During final predicton: do nothing; pretend there's no dropout\n",
    "  \n",
    "* Batch normalization\n",
    "  * During training, it substracts mean-over-batch and divides by std-over-batch and updates mean and variance.\n",
    "  * During final prediction, it uses accumulated mean and variance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
