{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mi primera red (MNIST,  clasificación de números con TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mnist_sample.png\" style=\"width:30%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print('Estamos usando TensorFlow ', tf.__version__)\n",
    "\n",
    "import sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib_utils\n",
    "from importlib import reload\n",
    "reload(matplotlib_utils)\n",
    "\n",
    "import keras_utils\n",
    "from keras_utils import reset_tf_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos\n",
    "\n",
    "Tenemos 50000 imágenes de 28x28 píxeles de números, del 0 al 9.\n",
    "Se entrenará un clasificador para estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessed_mnist\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X contiene valores en rgb, divididos entre 255 (normalizacion)\n",
    "\n",
    "print('X_train [dimensiones %s],  muestra:\\n' % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
    "print('\\nPlot de la muestra:')\n",
    "plt.imshow(X_train[1, 15:20, 5:10], cmap='Greys')\n",
    "plt.show();\n",
    "print('\\nUn número completo')\n",
    "plt.imshow(X_train[1], cmap='Greys')\n",
    "plt.show()\n",
    "print('y_train [dimensiones %s] 10 muestras:\\n' % (str(y_train.shape)), y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo lineal\n",
    "\n",
    "La tarea es entrenar un clasificador lineal $\\vec{x} \\rightarrow y$ con SGD usando TensorFlow.\n",
    "\n",
    "Necesitamos calcular una transformación lineal  $z_k$ para cada clase: \n",
    "$$z_k = \\vec{x} \\cdot \\vec{w_k} + b_k \\quad;k = 0..9$$\n",
    "\n",
    "Y transformar $z_k$ a probabilidades $p_k$ con softmax (regresión logistica): \n",
    "$$p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad;k = 0..9$$\n",
    "\n",
    "Como función de pérdida de usará la entropía cruzada (cross-entropy) para entrenar el clasificador multiclase:\n",
    "$$\\text{entropía cruzada}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n",
    "\n",
    "donde\n",
    "$$\n",
    "[x]=\\begin{cases}\n",
    "       1, \\quad \\text{si $x$ es cierto} \\\\\n",
    "       0, \\quad \\text{en el resto}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "La minimización de la entropía cruzada lleva $p_k$ cerca de 1 cuando $y = k$, que es lo que queremos.\n",
    "\n",
    "La hoja de ruta es:\n",
    "* Aplanar (flatten) las imágenes (28x28 -> 784) con `X_train.reshape((X_train.shape[0], -1))` para simplificar la implementación del modelo lineal.\n",
    "* Convertir `y_train` a vectores one-hot, 0 y 1, es lo necesario para la entropía cruzada.\n",
    "* Usar una variable `W` para todos los pesos (una columna $\\vec{w_k}$ por clase) y `b` para todos los sesgos.\n",
    "* Aspirar a ~0.93 de acierto en la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
    "print(X_train_flat.shape)\n",
    "\n",
    "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
    "print(X_val_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras  # solo se usará keras para keras.utils.to_categorical (one-hot)\n",
    "\n",
    "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_train_oh[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vuelve a ejecutar esto si rehaces el gráfico\n",
    "\n",
    "s = reset_tf_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros del modelo: W and b\n",
    "\n",
    "W = ### Tu codigo aqui ### tf.get_variable(...) con shape[0] = 784\n",
    "b = ### Tu codigo aqui ### tf.get_variable(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada de datos\n",
    "\n",
    "input_X = ### Tu codigo aqui ### \n",
    "#tf.placeholder(...) para X flat con shape[0] = None para cualquier tamaño de muestra (batch size)\n",
    "\n",
    "\n",
    "input_y = ### Tu codigo aqui ### tf.placeholder(...) para one-hot, etiquetas verdaderas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "\n",
    "logits = ### Tu codigo aqui ### ecuaciones lineales (logits) input_X, dimension resultante [input_X.shape[0], 10]\n",
    "probs = ### Tu codigo aqui ### aplicar tf.nn.softmax a logits\n",
    "clases = ### Tu codigo aqui ### aplicar tf.argmax para encontrar el indice de la clase con mas probabilidad\n",
    "\n",
    "# La perdida (loss) debe ser un escalar: la media de la perdida sobre todos los objetos con tf.reduce_mean().\n",
    "# Usar tf.nn.softmax_cross_entropy_with_logits en one-hot input_y, y sobre los logits.\n",
    "# Es identico a calcular la entropia cruzada (perdida) sobre las probabilidades\n",
    "perdida = ### Tu codigo aqui ### perdida (cross-entropy loss)\n",
    "\n",
    "# Usar por defecto tf.train.AdamOptimizer para obtener un paso en SGD\n",
    "paso = ### Tu codigo aqui ### optimizador que minimiza la perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "MUESTRA = 512  # tamaño de la muestra\n",
    "EPOCAS = 40\n",
    "\n",
    "# para seguir el proceso en Jupyter (para los que no tienen TensorBoard)\n",
    "curvas_entrenamiento = matplotlib_utils.SimpleTrainingCurves(\"cross-entropy\", \"accuracy\")\n",
    "\n",
    "for epoca in range(EPOCAS):  # se acaba una epoca cuando se ven todas la muestras de los datos\n",
    "    \n",
    "    perdidas_muestrales = []\n",
    "    for i in range(0, X_train_flat.shape[0], MUESTRA):  # los datos ya estan muestreados aleatoriamente\n",
    "        _, m_perd = s.run([paso, perdida], {input_X: X_train_flat[i: i+MUESTRA], \n",
    "                                                input_y: y_train_oh[i: i+MUESTRA]})\n",
    "        \n",
    "        perdidas_muestrales.append(m_perd)\n",
    "\n",
    "        \n",
    "    train_perd = np.mean(perdidas_muestrales)\n",
    "    val_perd = s.run(perdida, {input_X: X_val_flat, input_y: y_val_oh})  # esto suele ser pequeño\n",
    "    train_acierto = accuracy_score(y_train, s.run(clases, {input_X: X_train_flat}))  # lento y se suele pasar por alto\n",
    "    valid_acierto = accuracy_score(y_val, s.run(clases, {input_X: X_val_flat}))  \n",
    "    \n",
    "    curvas_entrenamiento.add(train_perd, val_perd, train_acierto, valid_acierto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Añadiendo más capas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a couple of hidden layers and see how that improves our validation accuracy!\n",
    "\n",
    "Rememeber that we need non-linearities for hidden layers (e.g. `tf.sigmoid`)!\n",
    "\n",
    "Now add 1 or more hidden layers to the code above and restart training.\n",
    "You're aiming for ~0.98 validation accuracy here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
